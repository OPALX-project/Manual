ifdef::env-gitlab[]
:stem: latexmath
:sectnums:
:toc: macro
:toclevels: 3

link:home[Back to Main Page]

toc::[]
endif::[]

[[chp.fieldsolvers]]
== Field Solver

Space charge effects are included in the simulation by specifying a
field solver described in this chapter and attaching it to the track
command as described in Chapter link:track#chp.track[Tracking]. By default, the code does not
assume any symmetry i.e. full 3D. In the near future it is planed to
implement also a slice (2D) model. This will allow the use of less
numbers of macro-particles in the simulation which reduces the
computational time significantly.

The space charge forces are calculated by solving the 3D Poisson
equation with open boundary conditions using a standard or integrated
Green function method. The image charge effects of the conducting
cathode are also included using a shifted Green function method. If more
than one Lorentz frame is defined, the total space charge forces are
then the summation of contributions from all Lorentz frames. More
details will be given in Version 2.0.0.

[[sec.fieldsolvers.fftbased]]
=== FFT Based Particle-Mesh (PM) Solver

The Particle-Mesh (PM) solver is one of the oldest improvements over the
PP solver. Still one of the best references is the book by R.W. Hockney
& J.W. Eastwood <<bib.hockney>>. The PM solver introduces a discretization of
space. The rectangular computation domain
latexmath:[\Omega:=[-L_x,L_x\]\times[-L_y,L_y\]\times[-L_t,L_t\]], just
big enough to include all particles, is segmented into a regular mesh of
latexmath:[M=M_x\times M_y\times M_t] grid points. For the discussion
below we assume latexmath:[N=M_x=M_y=M_t].

The solution of Poisson’s equation is an essential component of any
self-consistent electrostatic beam dynamics code that models the
transport of intense charged particle beams in accelerators. If the
bunch is small compared to the transverse size of the beam pipe, the
conducting walls are usually neglected. In such cases the Hockney method
may be employed <<bib.hockney>>, <<bib.eastwoodandbrownrigg>> and <<bib.hockneyandeastwood>>. In
that method, rather than computing latexmath:[N_p^2] point-to-point
interactions (where latexmath:[N_p] is the number of macro-particles),
the potential is instead calculated on a grid of size
latexmath:[(2 N)^d], where latexmath:[N] is the number of grid
points in each dimension of the physical mesh containing the charge, and
where latexmath:[d] is the dimension of the problem. Using the Hockney
method, the calculation is performed using Fast Fourier Transform (FFT)
techniques, with the computational effort scaling as
latexmath:[(2N)^d (log_2 2N)^d].

When the beam bunch fills a substantial portion of the beam pipe
transversely, or when the bunch length is long compared with the pipe
transverse size, the conducting boundaries cannot be ignored. Poisson
solvers have been developed previously to treat a bunch of charge in an
open-ended pipe with various geometries
<<bib.qiangandryne>> , <<bib.qiangandgluckstern>>.

The solution of the Poisson equation,

[latexmath]
++++
\nabla^2\phi=-\rho/\epsilon_0,
++++

for the scalar potential, latexmath:[\phi], due to a charge density,
latexmath:[\rho], and appropriate boundary conditions, can be
expressed as,

[latexmath]
++++
\phi(x,y,z)=\int\int\int{\mathrm{d}x' \,\mathrm{d}y' \,\mathrm{d}z'}\rho(x',y',z') G(x,x',y,y',z,z'),
++++

where latexmath:[G(x,x',y,y',z,z')] is the Green function, subject to
the appropriate boundary conditions, describing the contribution of a
source charge at location latexmath:[(x',y',z')] to the potential at
an observation location latexmath:[(x,y,z)].

For an isolated distribution of charge this reduces to

.Convolution solution
[latexmath#eq-convolutionsolution]
++++
\phi(x,y,z)=\int\int\int{\mathrm{d}x' \,\mathrm{d}y' \,\mathrm{d}z'}\rho(x',y',z') G(x-x',y-y',z-z'),
++++

where

[latexmath#eq-isolatedgreenfunction]
++++
G(u,v,w)={\frac{1}{\sqrt{u^2+v^2+w^2}}}.
++++

A simple discretization of <<eq-convolutionsolution>> on a Cartesian
grid with cell size latexmath:[(h_x,h_y,h_z)] leads to,

.Open brute force convolution
[latexmath#eq-openbruteforceconvolution]
++++
\phi_{i,j,k}=h_x h_y h_z \sum_{i'=1}^{M_x}\sum_{j'=1}^{M_y}\sum_{k'=1}^{M_t}  \rho_{i',j',k'}G_{i-i',j-j',k-k'},
++++

where latexmath:[\rho_{i,j,k}] and latexmath:[G_{i-i',j-j',k-k'}]
denote the values of the charge density and the Green function,
respectively, defined on the grid latexmath:[M].

[[sec.fieldsolvers.fft-based-convolutions-and-zero-padding]]
==== FFT-based Convolutions and Zero Padding

FFTs can be used to compute convolutions by appropriate zero-padding of
the sequences. Discrete convolutions arise in solving the Poisson
equation, and one is typically interested in the following,

.Brute force convolution
[latexmath#eq-bruteforceconvolution]
++++
\bar{\phi}_j=\sum_{k=0}^{K-1}\bar{\rho}_k \bar{G}_{j-k}\quad,
\begin{array}{l}
j=0,\ldots,J-1 \\
k=0,\ldots,K-1 \\
j-k=-(K-1),\ldots,J-1 \\
\end{array}
++++

where latexmath:[\bar{G}] corresponds to the free space Green
function, latexmath:[\bar{\rho}] corresponds to the charge density,
and latexmath:[\bar{\phi}] corresponds to the scalar potential. The
sequence latexmath:[\{\bar{\phi}_j\}] has latexmath:[J] elements,
latexmath:[\{\bar{\rho}_k\}] has latexmath:[K] elements, and
latexmath:[\{\bar{G}_m\}] has latexmath:[M=J+K-1] elements.

One can zero-pad the sequences to a length latexmath:[N\ge M] and use
FFT’s to efficiently obtain the latexmath:[\{\bar{\phi}_j\}] in the
unpadded region. This defines a zero-padded charge density,
latexmath:[\rho],

[latexmath]
++++
\rho_k=\left\{
\begin{array}{l l}
\bar{\rho}_k & \quad \text{if }k=0,\ldots,K-1 \\
0 & \quad \text{if }k=K,\ldots,N-1. \\
\end{array}\right.
++++

Define a periodic Green function,
latexmath:[G_m], as follows, 

.Periodic Green function
[latexmath#eq-periodicgreenfunction]
++++
G_m=\left\{
\begin{array}{l l}
\bar{G}_m & \quad \text{if }m=-(K-1),\ldots,J-1 \\
0 & \quad \text{if }m=J,\ldots,N-K, \\
G_{m+iN}=G_{m} & \quad \text{for } i \text{ integer }.
\end{array}\right.
++++

Now consider the sum

.FFT convolution
[latexmath#eq-fftconvolution]
++++
{\phi}_j=\frac{1}{N}\sum_{k=0}^{N-1} W^{-jk}
                    \left(\sum_{n=0}^{N-1} \rho_n W^{nk}\right)
                    \left(\sum_{m=0}^{N-1} G_m W^{mk}\right),
~~~~~~0 \le j \le N-1,
++++ 

where latexmath:[W=e^{-2\pi i/N}]. This
is just the FFT-based convolution of latexmath:[\{\rho_k\}] with
latexmath:[\{G_m\}]. Then, 

[latexmath]
++++
{\phi}_j=
          \sum_{n=0}^{K-1}~
          \sum_{m=0}^{N-1} \bar{\rho}_n G_m
\frac{1}{N}\sum_{k=0}^{N-1} W^{(m+n-j)k}
~~~~~~0 \le j \le N-1.
++++ 

Now use the relation

[latexmath]
++++
\sum_{k=0}^{N-1} W^{(m+n-j)k}= N \delta_{m+n-j,iN} ~ ~ ~ ~ ~(i~\mathrm{an~integer}).
++++

It follows that

[latexmath]
++++
{\phi}_j=\sum_{n=0}^{K-1}~\bar{\rho}_n G_{j-n+iN}
~~~~~~0 \le j \le N-1.
++++ 

But latexmath:[G] is periodic with period
latexmath:[N]. Hence,

.Final equation
[latexmath#eq-finaleqn]
++++
{\phi}_j=\sum_{n=0}^{K-1}~\bar{\rho}_n G_{j-n}
~~~~~~0 \le j \le N-1.
++++ 

In the physical (unpadded) region,
latexmath:[j\in \left[0,J-1\right\]], so the quantity latexmath:[j-n]
in <<eq-finaleqn>> satisfies latexmath:[-(K-1)\le j-n \le J-1]. In
other words the values of latexmath:[G_{j-n}] are identical to
latexmath:[\bar{G}_{j-n}]. Hence, in the physical region the FFT-based
convolution, <<eq-fftconvolution>>, matches the convolution in
<<eq-bruteforceconvolution>>.

As stated above, the zero-padded sequences need to have a length
latexmath:[N \ge M], where latexmath:[M] is the number of elements
in the Green function sequence latexmath:[\left\{x_m\right\}]. In
particular, one can choose latexmath:[N=M], in which case the Green
function sequence is not padded at all, and only the charge density
sequence, latexmath:[\left\{r_k\right\}], is zero-padded, with
latexmath:[k=0,\ldots,K-1] corresponding to the physical region and
latexmath:[k=K,\ldots,M-1] corresponding to the zero-padded region.

The above FFT-based approach – zero-padding the charge density array,
and circular-shifting the Green function in accordance with
<<eq-periodicgreenfunction>> – will work in general. In addition, if
the Green function is a symmetric function of its arguments, the value
at the end of the Green function array (at grid point latexmath:[J-1])
can be dropped, since it will be recovered implicitly through the
symmetry of <<eq-periodicgreenfunction>>. In that case the approach
is identical to the Hockney method <<bib.hockney>>, <<bib.eastwoodandbrownrigg>>
and <<bib.hockneyandeastwood>>. Lastly, note that the above
proof that the convolution, <<eq-fftconvolution>>, is identical to
<<eq-bruteforceconvolution>> in the unpadded region, works even when
latexmath:[W^{-j k}] and latexmath:[W^{m k}] are replaced by
latexmath:[W^{j k}] and latexmath:[W^{-m k}], respectively, in
<<eq-fftconvolution>>. In other words, the FFT-based approach can be
used to compute

.Brute force correlation
[latexmath#eq-bruteforcecorrelation]
++++
\bar{\phi}_j=\sum_{k=0}^{K-1}\bar{\rho}_k \bar{G}_{j+k}\quad,
\begin{array}{l}
j=0,\ldots,J-1 \\
k=0,\ldots,K-1 \\
j-k=-(K-1),\ldots,J-1 \\
\end{array}
++++ 

simply by changing the direction of
the Fourier transform of the Green function and changing the direction
of the final Fourier transform.

[[sec.fieldsolvers.algorithm-used-in-opal]]
==== Algorithm used in _OPAL_

As a result, the solution of <<eq-openbruteforceconvolution>> is
then given by

.Solution of brute force convolution
[latexmath#eq-oneterm]
++++
\phi_{i,j,k}=h_x h_y h_z \text{FFT}^{-1} \{ ( \text{FFT}\{\rho_{i,j,k}\}) ( \text{FFT}\{G_{i,j,k}\}) \}
++++

where the notation has been introduced that
latexmath:[\text{FFT}\{ . \}] denotes a forward FFT in all 3
dimensions, and latexmath:[\text{FFT}^{-1}\{ . \}] denotes a backward
FFT in all 3 dimensions.

[[sec.fieldsolvers.interpolation-schemes]]
==== Interpolation Schemes

More details will be given in Version 2.0.0.

[[sec.fieldsolvers.iterative-space-charge-solver]]
=== Iterative Space Charge Solver

This is a scalable parallel solver for the Poisson equation within a
Particle-In-Cell (PIC) code for the simulation of electron beams in
particle accelerators of irregular shape. The problem is discretized by
Finite Differences. Depending on the treatment of the Dirichlet boundary
the resulting system of equations is symmetric or `mildly' non-symmetric
positive definite. In all cases, the system is solved by the
preconditioned conjugate gradient algorithm with smoothed aggregation
(SA) based algebraic multigrid (AMG) preconditioning. More details are
given in <<bib.Adelmann2009p543>>.

[[sec.fieldsolvers.energy-binning]]
=== Energy Binning

The beam out of a cathode or in a plasma wake field accelerator can have
a large energy spread. In this case, the static approximation using one
Lorentz frame might not be sufficient. Multiple Lorentz frames can be
used so that within each Lorentz frame the energy spread is small and
hence the electrostatic approximation is valid. More details will be
given in Version 2.0.0.

[[sec.fieldsolvers.fieldsolvercmd]]
=== The `FIELDSOLVER` Command

See <<tab_FIELDSOLVER_Commands>> for a summary of the Fieldsolver command.

.Fieldsolver command summary
[[tab_FIELDSOLVER_Commands,Table {counter:tab-cnt}]]
[cols="<1,<4",options="header",]
|=======================================================================
|Command |Purpose
|`FIELDSOLVER` |Specify a fieldsolver

|`FSTYPE` |Specify the type of field solver: `FFT`, `FFTPERIODIC`, `MG` and `NONE`. Further
arguments are enabled with the AMR solver (cf. <<sec.fieldsolvers.AMR>>).

|`PARFFTX` |If `TRUE`, the dimension latexmath:[x] is distributed
among the processors

|`PARFFTY` |If `TRUE`, the dimension latexmath:[y] is distributed
among the processors

|`PARFFTZ` |If `TRUE`, the dimension latexmath:[z] is distributed
among the processors

|`MX` |Number of grid points in latexmath:[x] specifying rectangular
grid

|`MY` |Number of grid points in latexmath:[y] specifying rectangular
grid

|`MT` |Number of grid points in latexmath:[z] specifying rectangular
grid

|`BCFFTX` |Boundary condition in latexmath:[x] [`OPEN`]

|`BCFFTY` |Boundary condition in latexmath:[y] [`OPEN`]

|`BCFFTZ` |Boundary condition in latexmath:[z] [`OPEN,PERIODIC`]

|`GREENSF` |Defines the Greens function for the FFT Solver

|`BBOXINCR` |Enlargement of the bounding box in %

|`GEOMETRY` |Geometry to be used as domain boundary

|`ITSOLVER` |Type of iterative solver

|`INTERPL` |Interpolation used for boundary points

|`TOL` |Tolerance for iterative solver

|`MAXITERS` |Maximum number of iterations of iterative solver

|`PRECMODE` |Behavior of the preconditioner

|=======================================================================

[[sec.fieldsolvers.FSFSTYPE]]
=== Define the Fieldsolver to be used

At present only a FFT based solver is available. Future solvers will
include Finite Element solvers and a Multigrid solver with
Shortley-Weller boundary conditions for irregular domains.

[[sec.fieldsolvers.FSDomDEC]]
=== Define Domain Decomposition

The dimensions in latexmath:[x], latexmath:[y] and latexmath:[z]
can be parallel (`TRUE`) or serial `FALSE`. The default settings are:
parallel in latexmath:[z] and serial in latexmath:[x] and
latexmath:[y].

[[sec.fieldsolvers.FSMX]]
=== Define Number of Grid Points

Number of grid points in latexmath:[x], latexmath:[y] and
latexmath:[z] for a rectangular grid.

[[sec.fieldsolvers.FSBC]]
=== Define Boundary Conditions

Two boundary conditions can be selected independently among
latexmath:[x], latexmath:[y] namely: `OPEN` and for latexmath:[z]
`OPEN` & `PERIODIC`. In the case you select for latexmath:[z] periodic
you are about to model a DC-beam.

[[sec.fieldsolvers.FSGREEN]]
=== Define Greens Function

Two Greens functions can be selected: `INTEGRATED`, `STANDARD`. The
integrated Green’s function is described in <<bib.qiang2005_fieldsolver>>, <<bib.qiang2006-1_fieldsolver>>,
<<bib.qiang2006-2_fieldsolver>>. Default setting is `INTEGRATED`.

[[sec.fieldsolvers.FSBBOX]]
=== Define Bounding Box Enlargement

The bounding box defines a minimal rectangular domain including all
particles. With `BBOXINCR` the bounding box can be enlarged by a factor
given in percent of the minimal rectangular domain.

[[sec.fieldsolvers.GEOMETRY]]
=== Define Geometry

The list of geometries defining the beam line boundary. For further
details see Chapter link:geometry#chp.geometry[Geometry].

[[sec.fieldsolvers.ITSOLVER]]
=== Define Iterative Solver

The iterative solver for solving the preconditioned system: `CG`,
`BiCGSTAB` or `GMRES`.

[[sec.fieldsolvers.INTERPL]]
=== Define Interpolation for Boundary Points

The interpolation method for grid points near the boundary: `CONSTANT`,
`LINEAR` or `QUADRATIC`.

[[sec.fieldsolvers.TOL]]
=== Define Tolerance

The tolerance for the iterative solver used by the `MG` solver.

[[sec.fieldsolvers.MAXITERS]]
=== Define Maximal Iterations

The maximal number of iterations the iterative solver performs.

[[sec.fieldsolvers.PRECMODE]]
=== Define Preconditioner Behavior

The behavior of the preconditioner can be: `STD`, `HIERARCHY` or
`REUSE`. This argument is only relevant when using the `MG` solver and
should *only be set if the consequences to simulation and solver are
evident*. A short description is given in the
Table below.

.Preconditioner behavior command summary
[cols="<1,<4",options="header",]
[[tab_PRECMODE_Commands,Table {counter:tab-cnt}]]
|=======================================================================
|Value |Behavior
|`STD` |The preconditioner is rebuilt in every time step (enabled by
default)

|`HIERARCHY` |The hierarchy (tentative prolongator) is reused

|`REUSE` |The preconditioner is reused
|=======================================================================

[[sec.fieldsolvers.FSENBINS]]
=== Define the number of Energy Bins to use

Suppose latexmath:[\mathrm{d} E] the energy spread in the particle
bunch is to large, the electrostatic approximation is no longer valid.
One solution to that problem is to introduce latexmath:[k] energy bins
and perform latexmath:[k] separate field solves in which
latexmath:[\mathrm{d} E] is again small and hence the electrostatic
approximation valid. In case of a cyclotron see link:elements#sec.elements.cyclotron[Cyclotron] the
number of energy bins must be at minimum the number of neighboring
bunches (`NNEIGHBB`) i.e.
latexmath:[\mathrm{ENBINS} \le \mathrm{NNEIGHBB}].

The variable `MINSTEPFORREBIN` defines the number of integration step
that have to pass until all energy bins are merged into one.

[[sec.fieldsolvers.AMR]]
=== Define AMR Solver

After compiling _OPAL_ with `ENABLE_AMR=ON`, the option `AMR=TRUE` enables further
commands to be used in the Fieldsolver command (cf. <<tab_AMR_extensions_fieldsolver_command>>).

The current AMR interface in _OPAL_ is based on _AMReX_ <<bib.amrex>> which provides the
multigrid solvers `FMG` (till release 18.07) and `ML`. We also provide a _Trilinos_ based solver
which is described in the section <<sec.fieldsolvers.AMR.multigrid>>.

In order to setup an AMR simulation, the user has to specify the number of AMR levels. The
hierarchy is built based on the values of the refinement ratios, blocking factors and
maximum grid sizes of the base level. The maximum grid size of the next higher level is
given by the division with the refinement ratio. The same is true for
the blocking factors. The mesh refinement follows user-dependent rules that are
provided by `AMR_TAGGING`. The various mesh refinement methods are provided in
<<tab_Mesh_refinement_strategies>>.

.AMR extensions to the `FIELDSOLVER` command
[cols="<1,<4",options="header",]
[[tab_AMR_extensions_fieldsolver_command,Table {counter:tab-cnt}]]
|=======================================================================
|Command |Purpose
|`FSTYPE` | AMR Poisson solvers: `FMG` (_BoxLib_), `ML` (_AMReX_) and `AMR_MG`

|`AMR_MAXLEVEL` |Maximum adaptive mesh refinement level (single level if
`AMR_MAXLEVEL` is zero)

|`AMR_REFX` |Grid cell refinement ratio in x, only `AMR_REFX=2 is tested`

|`AMR_REFY` |Grid cell refinement ratio in y, only `AMR_REFY=2 is tested`

|`AMR_REFZ` |Grid cell refinement ratio in z, only `AMR_REFZ=2 is tested`

|`AMR_MAXGRIDX` |Maximum grid size of base level in x. It has to be smaller than
`MX` when running in parallel

|`AMR_MAXGRIDY` |Maximum grid size of base level in y. It has to be smaller than
`MY` when running in parallel

|`AMR_MAXGRIDZ` |Maximum grid size of base level in z. It has to be smaller than
`MZ` when running in parallel

|`AMR_BFX` |_AMReX_ blocking factor in x. `AMR_MAXGRIDX` has to be a multiple.

|`AMR_BFY` |_AMReX_ blocking factor in y. `AMR_MAXGRIDY` has to be a multiple.

|`AMR_BFZ` |_AMReX_ blocking factor in z. `AMR_MAXGRIDZ` has to be a multiple.

|`AMR_TAGGING` |Mesh-refinement strategy [`CHARGE_DENSITY, POTENTIAL, EFIELD, MOMENTA, MAX_NUM_PARTICLES, MIN_NUM_PARTICLES`]

|`AMR_DENSITY` | Charge density refinement threshold when `AMR_TAGGING=CHARGE_DENSITY`.
See also <<tab_Mesh_refinement_strategies>>.

|`AMR_MAX_NUM_PART` | Refinement threshold for `AMR_TAGGING=MAX_NUM_PARTICLES`.
See also <<tab_Mesh_refinement_strategies>>.

|`AMR_MIN_NUM_PART` | Refinement threshold for `AMR_TAGGING=MIN_NUM_PARTICLES`.
See also <<tab_Mesh_refinement_strategies>>.

|`AMR_SCALING` | Refinement threshold for `AMR_TAGGING=POTENTIAL` and `AMR_TAGGING=EFIELD`.
See also <<tab_Mesh_refinement_strategies>>.

|`AMR_DOMAIN_RATIO` | Box ratio of the AMR computation domain. It is an array of size 3.
The entries define the ratio in (x, y, z) direction. For example, latexmath:[[1, 0.75, 0.75\]] yields
a computation domain of latexmath:[[-1, 1\]\times [-0.75, 0.75\]\times [-0.75, 0.75\]].

|=======================================================================

.Mesh refinement strategies
[cols="<1,<4",options="header",]
[[tab_Mesh_refinement_strategies,Table {counter:tab-cnt}]]
|=======================================================================
|Value |Behavior
|`POTENTIAL` |Mark each cell if
latexmath:[\phi^{level}_{cell}\ge\alpha\max\phi^{level}], where
latexmath:[\alpha\in[0, 1\]] is the scaling factor `AMR_SCALING`

|`EFIELD` |Mark each cell if the electric field component of any
direction satisfies
latexmath:[E^{level}_{d, cell}\ge\alpha\max E_{d}^{level}], where
latexmath:[d=x, y, z] and latexmath:[\alpha\in[0, 1\]] is the scaling
factor `AMR_SCALING`

|`MOMENTA` |It performs a loop over all particles of a level and
computes the dot product of the momenta. Every cell that contains a
particle with
latexmath:[\|\|\mathbf{p}\|\| \ge \alpha \max_{level} \|\|\mathbf{p}\|\|] is
refined. The scalar latexmath:[\alpha\in[0, 1\]] is a user-defined
value `AMR_SCALING`.

|`CHARGE_DENSITY` |If the charge density of a cell is greater or equal
to the value specified with `AMD_DENSITY` the cell is tagged for
refinement

|`MIN_NUM_PARTICLES` |Cells with equal or more particles are
refined. The bound is specified with `AMR_MIN_NUM_PART`

|`MAX_NUM_PARTICLES` |Cells with equal or less particles are
refined. The bound is specified with `AMR_MAX_NUM_PART`
|=======================================================================

[[sec.fieldsolvers.AMR.multigrid]]
==== Hardware-Architecture Independent AMR Poisson Solver

This solvers is enabled with `ENABLE_AMR_MG_SOLVER=ON` and requires a working
_Trilinos_ installation with at least the following packages:

* _Tpetra_ <<bib.tpetra>>
* _Ifpack2_ <<bib.ifpack2>>
* _Amesos2_ and _Belos_ <<bib.amesos2-belos>>
* _MueLu_ <<bib.muelu>>

Some base level linear solvers may require additional third party libraries. These
are `SUPERLU` <<bib.superlu>>, `UMFPACK` <<bib.umfpack>>, `PARDISO_MKL` <<bib.pardiso>> <<bib.pardiso-2>>,
`MUMPS` <<bib.mumps>> and `LAPACK` <<bib.lapack>>. A full list of the arguments
and linear solvers is given in <<tab_AMR_MG_Commands>>. The solver is tested with
version 12.14.1. The corresponding paper is <<bib.amrsolver>>.

.Extension of the `FIELDSOLVER` command for the AMR Poisson Solver
[cols="<1,<4",options="header",]
[[tab_AMR_MG_Commands,Table {counter:tab-cnt}]]
|=======================================================================
|Command |Purpose
|`AMR_MG_SMOOTHER` |The pre- and post-smoother which is either
[`GS` (Gauss-Seidel), `JACOBI` or `SGS` (symmetric Gauss-Seidel)]. Default: `GS`

|`AMR_MG_NSWEEPS` |The number of smoothing steps. Default: 8

|`AMR_MG_PREC` |The preconditioner of the bottom linear solver. Please see
<<bib.ifpack2>> for further information.
[`NONE, ILUT, CHEBYSHEV, RILUK, SA, JACOBI, BLOCK_JACOBI, GS, BLOCK_GS`]. Default: `NONE`

|`AMR_MG_INTERP` |The prolongator of the level solution to next higher level
[`PC` (piecewise constant), `TRILINEAR`, `LAGRANGE` (coarse-fine interface only)].
Default: `PC`

|`AMR_MG_NORM` |The norm of the convergence criterion [`L1, L2, LINF`]. Default: `LINF`

|`AMR_MG_VERBOSE` |Boolean to enable/disable solver output. It writes an SDDS file.
Default: `FALSE`

|`AMR_MG_REBALANCE` |Boolean to rebalance the smoothed aggregation preconditioner or
linear solver (if `TRUE`, it uses subcommunicators to reduce the communication
overhead). Default: `FALSE`

|`AMR_MG_REBALANCE` |Reuse type of the smoothed aggregation multigrid solver of MueLu.
Please see <<bib.muelu>> for further information.
[`NONE, RP, RAP, S, FULL`]. Default: `RAP`

|`AMR_MG_TOL` |The tolerance of the solver. Default: latexmath:[10^{-10}]

|`ITSOLVER` |The solver of the linear system of equations on the base level.
[`BICGSTAB, MINRES, PCPG, CG, GMRES, STOCHASTIC_CG, RECYCLING_CG, RECYCLING_GMRES, KLU2,
SUPERLU, UMFPACK, PARDISO_MKL, MUMPS, LAPACK, SA`]. Please see
<<bib.amesos2-belos>> for further information. Default: `CG`
|=======================================================================


[[sec.fieldsolvers.AMR.use]]
==== Use of AMR
AMR is only available in the multi-bunch mode (cf. <<sec.opalcycl.MultiBunch>>) of _OPAL-cycl_. As soon as
more than one bunch is in the simulation, the AMR hierarchy is built. Note that _AMReX_ handles the MPI parallelism,
hence, other Poisson solvers of _OPAL_ (cf. `FSTYPE` in <<tab_FIELDSOLVER_Commands>>) are not available.

[[sec.fieldsolvers.bibliography]]
=== References

anchor:bib.hockney[[{counter:bib-cnt}\]]
<<bib.hockney>> R. W. Hockney, _The potential calculation and some applications_, Methods Comput. Phys. 9, 136–210 (1970).

anchor:bib.eastwoodandbrownrigg[[{counter:bib-cnt}\]]
<<bib.eastwoodandbrownrigg>> J. W. Eastwood and D. R. K. Brownrigg, _Remarks on the solution of poisson’s equation for isolated systems_, J. Comput. Phys. 32, 24–38 (1979).

anchor:bib.hockneyandeastwood[[{counter:bib-cnt}\]]
<<bib.hockneyandeastwood>> R. W. Hockney and J. W. Eastwood, _Computer simulation using particles_ (Taylor & Francis Group, 1988).

anchor:bib.qiangandryne[[{counter:bib-cnt}\]]
<<bib.qiangandryne>> J. Qiang and R. Ryne, _Parallel 3d poisson solver for a charged beam in a conducting pipe_, Comput. Phys. Commun. 138, 18–28 (2001).

anchor:bib.qiangandgluckstern[[{counter:bib-cnt}\]]
<<bib.qiangandgluckstern>> J. Qiang and R. L. Gluckstern, _Three-dimensional poisson solver for a charged beam with large aspect ratio in a conducting pipe_, Comput. Phys. Commun. 160, 120–128 (2004).

anchor:bib.Adelmann2009p543[[{counter:bib-cnt}\]]
<<bib.Adelmann2009p543>> A. Adelmann, P. Arbenz, and Y. Ineichen, https://arxiv.org/abs/0907.4863[_A fast parallel poisson solver on irregular domains applied to beam dynamic simulations_], arXiv (2009).

anchor:bib.qiang2005_fieldsolver[[{counter:bib-cnt}\]]
<<bib.qiang2005_fieldsolver>> J. Qiang et al.,  http://repositories.cdlib.org/lbnl/LBNL-59098[_A three-dimensional quasi-static model for high brightness beam dynamics simulation_], tech. rep. LBNL-59098 (LBNL).

anchor:bib.qiang2006-1_fieldsolver[[{counter:bib-cnt}\]]
<<bib.qiang2006-1_fieldsolver>> J. Qiang et al., _Three-dimensional quasi-static model for high brightness beam dynamics simulation_, Phys. Rev. ST Accel. Beams 9 (2006).

anchor:bib.qiang2006-2_fieldsolver[[{counter:bib-cnt}\]]
<<bib.qiang2006-2_fieldsolver>> J. Qiang et al., _Erratum: three-dimensional quasi-static model for high brightness beam dynamics simulation_, Phys. Rev. ST Accel. Beams 10, 12990 (2007).

anchor:bib.amrex[[{counter:bib-cnt}\]]
<<bib.amrex>> W. Zhang, A. Almgren, et al., https://doi.org/10.21105/joss.01370[_AMReX: a framework for block-structured adaptive mesh refinement_], Journal of Open Source Software, 4(37):1370 (2019).

anchor:bib.tpetra[[{counter:bib-cnt}\]]
<<bib.tpetra>> C. G. Baker and M. A. Heroux, https://doi.org/10.3233/SPR-2012-0349[_Tpetra, and the use of generic programming in scientific computing_], Scientific Programming, 20(2):115–128 (2012).

anchor:bib.ifpack2[[{counter:bib-cnt}\]]
<<bib.ifpack2>> A. Prokopenko, C. M. Siefert, J. J. Hu, M. Hoemmen, and A. Klinvex, https://doi.org/10.2172/1259544[_Ifpack2 User's Guide 1.0_], Technical Report SAND2016-5338, Sandia National Labs (2016).

anchor:bib.amesos2-belos[[{counter:bib-cnt}\]]
<<bib.amesos2-belos>> E. Bavier, M. Hoemmen, S. Rajamanickam, and H. Thornquist, https://doi.org/10.3233/SPR-2012-0352[_Amesos2 and Belos: Direct and iterative solvers for large sparse linear systems_], Scientific Programming, 20(3):241–255 (2012).

anchor:bib.muelu[[{counter:bib-cnt}\]]
<<bib.muelu>> L. Berger-Vergiat, C. A. Glusa, J. J. Hu, M. Mayr, A. Prokopenko, C. M. Siefert, R. S. Tuminaro, and T. A. Wiesner, https://doi.org/10.2172/1491860[_MueLu User's Guide_], Technical Report SAND2019-0537, Sandia National Laboratories (2019).

anchor:bib.superlu[[{counter:bib-cnt}\]]
<<bib.superlu>> X. Li, J. Demmel, J. Gilbert, L. Grigori, M. Shao, and I. Yamazaki, http://crd.lbl.gov/~xiaoye/SuperLU[_SuperLU Users' Guide_], Technical Report LBNL-44289, Lawrence Berkeley National Laboratory (1999).

anchor:bib.umfpack[[{counter:bib-cnt}\]]
<<bib.umfpack>> T. A. Davis, https://doi.org/10.1145/992200.992206[_Algorithm 832: UMFPACK V4.3—an Unsymmetric-Pattern Multifrontal Method_], ACM Trans. Math. Softw., 30(2):196–199 (2004).

anchor:bib.pardiso[[{counter:bib-cnt}\]]
<<bib.pardiso>> O. Schenk and K. Gärtner, https://doi.org/10.1016/j.future.2003.07.011[_Solving unsymmetric sparse systems of linear equations with PARDISO_], Future Generation Computer Systems, 20(3):475–487 (2004).

anchor:bib.pardiso-2[[{counter:bib-cnt}\]]
<<bib.pardiso-2>> https://software.intel.com/en-us/onemkl-linux-developer-guide[_Developer Guide for Intel(R) oneAPI Math Kernel Library for Linux_], last updated: February 7, 2020.

anchor:bib.mumps[[{counter:bib-cnt}\]]
<<bib.mumps>> P. R. Amestoy, I. S. Duff, J.-Y. L'Excellent, and J. Koster, https://doi.org/10.1137/S0895479899358194[_A Fully Asynchronous Multifrontal Solver Using Distributed Dynamic Scheduling_], SIAM Journal on Matrix Analysis and Applications, 23(1):15–41 (2001).

anchor:bib.lapack[[{counter:bib-cnt}\]]
<<bib.lapack>> E. Anderson, Z. Bai, C. Bischof, L. S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen, https://doi.org/10.1137/1.9780898719604[_LAPACK Users' Guide_], SIAM, third edition, 1999,
ISBN 978-0-89871-447-0.

anchor:bib.amrsolver[[{counter:bib-cnt}\]]
<<bib.amrsolver>> M. Frey, A. Adelmann, and U. Locans, https://doi.org/10.1016/j.cpc.2019.106912[_On architecture and performance of adaptive mesh refinement in an electrostatics Particle-In-Cell code_], Computer Physics Communications, 247:106912 (2020).

// EOF
